# IO模型-阻塞、非阻塞、同步和异步

## 一、引言

**同步异步I/O**，**阻塞非阻塞I/O**是程序员老生常谈的话题了，也是自己一直以来懵懵懂懂的一个话题。比如：<u>何为同步异步？何为阻塞与非阻塞？二者的区别在哪里？阻塞在何处？为什么会有多种IO模型，分别用来解决问题？常用的框架采用的是何种I/O模型？各种IO模型的优劣势在哪里，适用于何种应用场景？</u>

简而言之，对于I/O的认知，不能仅仅停留在字面上认识，了解内部玄机，才能深刻理解I/O，才能看清I/O相关问题的本质。



## 二、I/O 的定义

I/O（**I**nput/**O**utput） 即**输入／输出** 。

### 1. 计算机视角

**我们先从计算机结构的角度来解读一下 I/O。**

根据冯.诺依曼结构，计算机结构分为 5 大部分：运算器、控制器、存储器、输入设备、输出设备。

![](assets/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303139303632343132323132363339382e6a7065673f782d6f73732d70726f636573733d696d6167652f77617465726d61726b2c747970655f5a6d46755a33706f5a57356e6147567064476b2c7-16287673145013.jfif)

输入设备（比如键盘）和输出设备（比如鼠标）都属于外部设备。网卡、硬盘这种既可以属于输入设备，也可以属于输出设备。

输入设备向计算机输入数据，输出设备接收计算机输出的数据。

**从计算机结构的视角来看的话， I/O 描述了计算机系统与外部设备之间通信的过程。**

所以I/O之于计算机，有两层意思：

1. I/O设备
2. 对I/O设备的数据读写

对于一次I/O操作，必然涉及2个参与方，一个输入端，一个输出端，而又根据参与双方的设备类型，我们又可以分为==**磁盘I/O**==，==**网络I/O**==（一次网络的请求响应，网卡）等。

### 2. 程序视角

应用程序作为一个文件保存在磁盘中，只有加载到内存到成为一个进程才能运行。应用程序运行在计算机内存中，必然会涉及到数据交换，比如读写磁盘文件，访问数据库，调用远程 API 等等。但我们编写的程序并不能像操作系统内核一样直接进行I/O操作。

因为为了确保操作系统的安全稳定运行，操作系统启动后，将会开启保护模式：将内存分为**内核空间**（也称为**内核态**，内核对应进程所在内存空间，控制计算机的硬件资源，并提供上层应用程序运行的环境）和**用户空间**（也称为**用户态**，即上层应用程序的活动空间，应用程序的执行必须依赖于内核提供的资源），进行内存隔离。我们构建的程序将运行在用户空间，用户空间无法操作内核空间，也就意味着用户空间的程序不能直接访问由内核管理的I/O，比如：硬盘、网卡等。

但操作系统向外提供API，其由各种类型的**系统调用（System Call）**组成，以提供安全的访问控制。所以应用程序要想访问内核管理的I/O，必须通过调用内核提供的**系统调用(system call）**进行**间接访问**。

所以I/O之于应用程序来说，强调的通过**向内核发起系统调用完成对I/O的间接访问**。换句话说应用程序发起的一次IO操作实际包含两个阶段：

1. **IO调用阶段**：应用程序进程向内核发起系统调用
2. **IO执行阶段**：内核执行IO操作并返回
   2.1. **准备数据阶段**：内核等待I/O设备准备好数据
   2.2. **拷贝数据阶段**：将数据从内核缓冲区拷贝到用户空间缓冲区

怎么理解准备数据阶段呢？
对于写请求：等待系统调用的完整请求数据，并写入内核缓冲区；
对于读请求：等待系统调用的完整请求数据；（若请求数据不存在于内核缓冲区）则将外围设备的数据读入到内核缓冲区。

![](assets/17175f198660204b)

而应用程序进程在发起IO调用至内核执行IO返回之前，应用程序进程/线程所处状态，就是我们下面要讨论的第二个话题**阻塞IO与非阻塞IO**。

## 三、IO 模型之阻塞I/O(BIO)

应用程序中进程在发起IO调用后至内核执行IO操作返回结果之前，若发起系统调用的线程一直处于等待状态，则此次IO操作为**阻塞IO**。阻塞IO简称**BIO（Blocking IO）**。其处理流程如下图所示：

![](assets/17175f198678bcde)

从上图可知当用户进程发起IO系统调用后，内核从准备数据到拷贝数据到用户空间的两个阶段期间**用户调用线程选择阻塞等待**数据返回。

**阻塞IO在两个阶段都是阻塞**！！

因此BIO带来了一个问题：如果内核数据需要耗时很久才能准备好，那么用户进程将被阻塞，浪费性能。为了提升应用的性能，虽然可以通过多线程来提升性能，但线程的创建依然会借助系统调用，同时多线程会导致频繁的线程上下文的切换，同样会影响性能。所以要想解决BIO带来的问题，我们就得看到问题的本质，那就是**阻塞**二字。

## 四、IO 模型之非阻塞I/O(NIO)

那解决方案自然也容易想到，将阻塞变为非阻塞，那就是用户进程在发起系统调用时指定为非阻塞，内核接收到请求后，就会立即返回，然后用户进程通过轮询的方式来拉取处理结果。也就是如下图所示：

![](assets/17175f198687b2b3)

应用程序中进程在发起IO调用后至内核执行IO操作返回结果之前，若发起系统调用的线程不会等待而是立即返回，则此次IO操作为非阻塞IO模型。非阻塞IO简称NIO，Non-Blocking IO。

**非阻塞IO和阻塞IO相似，区别在于第一阶段，在等待数据准备就绪的过程中，用户进程不会阻塞。**

然而，非阻塞IO虽然相对于阻塞IO大幅提升了性能，但依旧不是完美的解决方案，其依然存在性能问题，也就是**频繁的轮询**导致频繁的系统调用，会耗费大量的CPU资源。比如当并发很高时，假设有1000个并发，那么单位时间循环内将会有1000次系统调用去轮询执行结果，而实际上可能只有2个请求结果执行完毕，这就会有998次无效的系统调用，造成严重的性能浪费。有问题就要解决，那**NIO问题的本质就是频繁轮询导致的无效系统调用**。

## 五、IO模型之IO多路复用

解决NIO的思路就是**降解无效的系统调用**，如何降解呢？我们一起来看看以下几种IO多路复用的解决思路。

### 1. IO多路复用之select/poll

Select是内核提供的系统调用，它支持**一次查询多个系统调用的可用状态**（**一次轮询多个socket**），当任意一个结果状态可用时就会返回，用户进程再发起一次系统调用进行数据读取。换句话说，就是**NIO中N次的系统调用，借助Select，只需要发起一次系统调用**就够了。其IO流程如下所示：

![](assets/17175f198bf13c57)

**IO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用。read 调用的过程（数据从内核空间->用户空间）还是阻塞的。**

**IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。**

但是，select有一个限制，就是**存在连接数限制**，针对于此，又提出了poll。其与select相比，主要是解决了**连接限制**。

select/poll 虽然解决了NIO重复无效系统调用用的问题，但同时又引入了新的问题。问题是：

1. 用户空间和内核空间之间，大量的数据拷贝
2. 内核循环遍历IO状态，浪费CPU时间

换句话说，select/poll虽然减少了用户进程的发起的系统调用，但内核的工作量只增不减。在高并发的情况下，内核的性能问题依旧。所以select/poll的问题本质是：**内核存在无效的循环遍历**。

### 2. IO多路复用之epoll

针对select/pool引入的问题，我们把解决问题的思路转回到内核上，**如何减少内核重复无效的循环遍历呢？**变主动为被动，**基于事件驱动**来实现。其流程图如下所示：

![](assets/17175f198d29b33a)

epoll相较于select/poll，多了两次系统调用，其中epoll_create建立与内核的连接，epoll_ctl注册事件，epoll_wait阻塞用户进程，等待IO事件。

![](assets/17175f198e87ddcc)

epoll，已经大大优化了IO的执行效率，但在IO执行的第一阶段：**等待数据准备阶段都还是被阻塞的**。所以这是一个可以继续优化的点。



## 六、IO 模型之信号驱动IO(SIGIO)

信号驱动IO与BIO和NIO最大的区别就在于，**在IO执行的数据准备阶段，不会阻塞用户进程**。

如下图所示：当用户进程需要等待数据的时候，会向内核发送一个信号，告诉内核我要什么数据，然后用户进程就继续做别的事情去了，而当内核中的数据准备好之后，内核立马发给用户进程一个信号，说”数据准备好了，快来查收“，用户进程收到信号之后，立马调用recvfrom，去查收数据。

![](assets/17175f19b6c42f1c)

乍一看，信号驱动式I/O模型有种异步操作的感觉，但是**在IO执行的第二阶段，也就是将数据从内核空间复制到用户空间这个阶段，用户进程还是被阻塞的**。

综上，你会发现，**不管是BIO还是NIO还是SIGIO，它们最终都会被阻塞在IO执行的第二阶段**。
那如果能将IO执行的第二阶段变成非阻塞，那就完美了。



## 七、IO 模型之异步IO(AIO)

异步IO（Asynchronous I/O）真正实现了IO全流程的非阻塞。用户进程发出系统调用后立即返回，内核等待数据准备完成，然后将数据拷贝到用户进程缓冲区，然后发送信号告诉用户进程**IO操作执行完毕**（与SIGIO相比，一个是发送信号告诉用户进程数据准备完毕，一个是IO执行完毕）。其流程如下：

![](assets/17175f19b8e85e2b)

异步 IO 是**基于事件和回调机制**实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作。

所以，**之所以称为异步IO，取决于IO执行的第二阶段是否阻塞。因此前面讲的BIO，NIO和SIGIO均为同步IO**。



## 八、总结

![](assets/17175f19bb874153)

一个IO操作其实分成了两个步骤：**发起IO请求**和**实际的IO操作**（实际的IO操作又分为两步：**等待数据准备**、**将数据从内核缓冲区拷贝到用户空间缓冲区**）。

阻塞IO和非阻塞IO的区别在于第一步（发起IO请求），发起IO请求是否会被阻塞，如果阻塞直到完成那么就是传统的阻塞IO，如果不阻塞，那么就是非阻塞IO。

同步IO和异步IO的区别就在于第二个步骤是否阻塞：如果实际的IO读写阻塞请求进程，那么就是同步IO，因此阻塞IO、非阻塞IO、IO复用、信号驱动IO都是同步IO；如果不阻塞，而是操作系统帮你做完IO操作再将结果返回给你，那么就是异步IO。

- 同步阻塞I/O：
  - 当进程调用某些设计I/O操作的系统调用或库函数时，比如accept()、send()、recv()等，进程便暂停下来，等待I/O操作完成后再继续运行。
- 同步非阻塞I/O：（轮询）
  - 不会等待数据就绪，而是结合反复轮询来尝试数据是否就绪。
  - 与同步阻塞I/O相比，**同步非阻塞I/O好处是在一个进程中可以同时处理多个I/O操作，而不是阻塞在一个I/O操作上**
- 多路I/O就绪通知：（I/O复用）
  - **允许进程通过一种方法来同时监听所有文件描述符，并可以快速获得所有就绪的文件描述符，然后只针对这些文件描述符进行数据访问。**我们常用的select、poll、epoll等函数使用了I/O复用模型。
  - 与同步非阻塞I/O相比，I/O复用模型的优势在于**可以同时等待多个（而不只是一个）套接字描述符就绪**
- 信号驱动式I/O：
  - 需要开启套接字的信号驱动I/O功能，并通过sigaction系统调用安装一个信号处理函数。sigaction函数立即返回，我们的进程继续工作，即进程没有被阻塞。当数据报准备好时，内核会为该进程产生一个SIGIO信号，这样我们可以在信号处理函数中调用recvfrom读取数据报，也可以在主循环中读取数据报。无论如何处理SIGIO信号，这种模型的优势在于等待数据报到达期间不被阻塞。
- 异步I/O(AIO)：
  - 启动某个操作，并让内核在整个操作（包括等待数据和将数据从内核复制到用户空间）完成后通知应用进程。
  - 与信号驱动式I/O的区别在于：信号驱动式I/O在数据报准备好时就通知应用进程，应用进程还需要将数据报从内核复制到用户进程缓冲区；而异步I/O模型则是整个操作完成才通知应用进程，应用进程在整个操作期间都不会被阻塞。



## 问题

有人说 "阻塞模式挺好的, 应为在阻塞状态下, 用户进程会被挂起, 挂起就是说不会再占用CPU资源了"

我觉着 阻塞模型这不挺好么, 自己所请求的网络数据没有准备好, 然后把CPU让给别人用, 这不是很好么?

又有些人说 "非阻塞好, 非阻塞可以在用户进程请求的数据没有准备好的时候, 让内核立即给予响应, 然后用户进程可以干别的, 一会儿再来检查一下, 这样轮循" 

我觉着这样也没问题啊, 至少进程也没闲着啊, 只不过 进程虽然干了别的, 但也多干了一些活,例如轮询

相比起来, 貌似比阻塞省下了CPU资源给别人用; 而非阻塞是抓紧利用CPU资源, 但为了抓紧使用，缺同时走了不少多余的路,，比如不断地轮询 所以？？？？？？

```
假设你用 php-fpm，你的 php 程序中需要向外部接口请求。php-fpm 是阻塞的模型，那么每一个 Worker 进程在执行这些网络 I/O 的时候，是不是都阻塞了？假设你的 php-fpm 最大进程数有 500 个，那么同时进来了 500 个请求，是不是都阻塞在了网络 I/O 上了？那么接下来，php-fpm 已经无法处理第 501 个请求了。可是此时，由于在等待网络 I/O 响应，CPU 实际上并没有做什么工作，你会发现，CPU 闲的要死，但是却无法处理请求了。

那么非阻塞呢？用 Swoole 举例子。我们在网络 I/O 的时候，让它去等待响应，与此同时，处理下一个请求。那么，我们会发现，并发数上去了，CPU 的利用率变高了。

假设你在用 Redis 的时候，它返回数据是毫秒级别的，那么你认为，它是阻塞呢还是非阻塞呢？这个时候，这两个的概念就模糊了。具体你还是要实际判断。
```

```
如果只有一个套接字的情况下，使用阻塞IO是极好的，读到数据就返回。

但是如果在有很多套接字的情况下，比如有100个套接字:

如果使用阻塞IO，可能因为读取一个没有数据的套接字而阻塞剩下的99个套接字的数据处理，那么就会造成服务器的响应性很差。
如果使用非阻塞IO，那么就需要轮询这一百个套接字到底可不可以读取到数据，这个轮询操作会浪费CPU时间片，照样也不是一个高效的方式，套接字多了，照样性能很差。

那有没有一种比较好的方式来同时检测多个套接字是否可读可写，并且不浪费CPU时间片呢？那就是要用IO多路复用了，使用IO多路复用可以同时检测多个不同的套接字是否就绪。有多种IO多路复用的实现，其中包括select，poll, epoll, /dev/poll, kqueue等。
```

## 阅读原文

- JavaGuide IO 模型：[https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/basis/IO模型.md](https://github.com/Snailclimb/JavaGuide/blob/master/docs/java/basis/IO模型.md)


- IO 模型知多少 | 理论篇：https://www.cnblogs.com/sheng-jie/p/how-much-you-know-about-io-models.html


- 什么是阻塞，非阻塞，同步，异步？：https://sowhatbigfatloser.com/shi-yao-shi-zu-sai-fei-zu-sai-tong-bu-yi-bu/
- 阻塞和非阻塞 都有优点, 我理解的对么：https://segmentfault.com/q/1010000010415760



------

==**追加：**==上面的看的有一点明白，但也有一些东西迷迷糊糊的，下文解决了很多我迷惑的问题

------



# 结合代码聊聊 Java 网络编程中的 BIO、NIO 和 AIO

## 一、到底什么是“IO Block”

很多人说BIO不好，会“block”，但到底什么是IO的Block呢？考虑下面两种情况：

- 用系统调用`read`从socket里读取一段数据
- 用系统调用`read`从一个磁盘文件读取一段数据到内存

如果你的直觉告诉你，这两种都算“Block”，那么很遗憾，你的理解与Linux不同。Linux认为：

- 对于第一种情况，算作block，因为Linux无法知道网络上对方是否会发数据。如果没数据发过来，对于调用`read`的程序来说，就只能“等”。
- 对于第二种情况，**不算做block**。

是的，**对于磁盘文件IO，Linux总是不视作Block**。

你可能会说，这不科学啊，磁盘读写偶尔也会因为硬件而卡壳啊，怎么能不算Block呢？但实际就是不算。

> 一个解释是，所谓“Block”是指操作系统可以预见这个Block会发生才会主动Block。例如当读取TCP连接的数据时，如果发现Socket buffer里没有数据就可以确定定对方还没有发过来，于是Block；而对于普通磁盘文件的读写，也许磁盘运作期间会抖动，会短暂暂停，但是操作系统无法预见这种情况，只能视作不会Block，照样执行。

基于这个基本的设定，在讨论IO时，一定要严格区分网络IO和磁盘文件IO。==**NIO和IO多路复用只对网络IO有意义**==。

> 严格的说，O_NONBLOCK和IO多路复用，对标准输入输出描述符、管道和FIFO也都是有效的。但本文侧重于讨论高性能网络服务器下各种IO的含义和关系，所以本文做了简化，只提及网络IO和磁盘文件IO两种情况。

下文先着重讲一下**网络IO**。

## 二、BIO

有了Block的定义，就可以讨论BIO和NIO了。BIO是Blocking IO的意思。在类似于网络中进行`read`, `write`, `connect`一类的系统调用时会被卡住。

举个例子，当用`read`去读取网络的数据时，是无法预知对方是否已经发送数据的。因此在收到数据之前，能做的只有等待，直到对方把数据发过来，或者等到网络超时。

对于单线程的网络服务，这样做就会有卡死的问题。因为当等待时，整个线程会被挂起，无法执行，也无法做其他的工作。

> 顺便说一句，这种Block是不会影响同时运行的其他程序（进程）的，因为现代操作系统都是多任务的，任务之间的切换是抢占式的。这里**Block只是指Block当前的进程**。

于是，网络服务为了同时响应多个并发的网络请求，必须实现为多线程的。每个线程处理一个网络请求。线程数随着并发连接数线性增长。这的确能奏效。实际上2000年之前很多网络服务器就是这么实现的。但这带来两个问题：

- 线程越多，Context Switch就越多，而Context Switch是一个比较重的操作，会无谓浪费大量的CPU。
- 每个线程会占用一定的内存作为线程的栈。比如有1000个线程同时运行，每个占用1MB内存，就占用了1个G的内存。

> 也许现在看来1GB内存不算什么，现在服务器上百G内存的配置现在司空见惯了。但是倒退20年，1G内存是很金贵的。并且，尽管现在通过使用大内存，可以轻易实现并发1万甚至10万的连接。但是水涨船高，如果是要单机撑1千万的连接呢？

问题的关键在于，当调用`read`接受网络请求时，有数据到了就用，没数据到时，实际上是可以干别的。使用大量线程，仅仅是因为Block发生，没有其他办法。

当然你可能会说，是不是可以弄个线程池呢？这样既能并发的处理请求，又不会产生大量线程。但这样会限制最大并发的连接数。比如你弄4个线程，那么最大4个线程都Block了就没法响应更多请求了。

要是操作IO接口时，操作系统能够总是直接告诉有没有数据，而不是Block去等就好了。于是，NIO登场。

## 三、NIO

NIO是指将IO模式设为“Non-Blocking”模式。在Linux下，一般是这样：

```c
void setnonblocking(int fd) {
    int flags = fcntl(fd, F_GETFL, 0);
    fcntl(fd, F_SETFL, flags | O_NONBLOCK);
}
```

> 再强调一下，以上操作只对socket对应的文件描述符有意义；对磁盘文件的文件描述符做此设置总会成功，但是会直接被忽略。

这时，BIO和NIO的区别是什么呢？

在BIO模式下，调用read，如果发现没数据已经到达，就会Block住。

在NIO模式下，调用read，如果发现没数据已经到达，就会立刻返回-1, 并且errno被设为`EAGAIN`。

> 在有些文档中写的是会返回`EWOULDBLOCK`。实际上，在Linux下`EAGAIN`和`EWOULDBLOCK`是一样的，即`#define EWOULDBLOCK EAGAIN`

于是，一段NIO的代码，大概就可以写成这个样子。

```c
struct timespec sleep_interval{.tv_sec = 0, .tv_nsec = 1000};
ssize_t nbytes;
while (1) {
    /* 尝试读取 */
    if ((nbytes = read(fd, buf, sizeof(buf))) < 0) {
        if (errno == EAGAIN) { // 没数据到
            perror("nothing can be read");
        } else {
            perror("fatal error");
            exit(EXIT_FAILURE);
        }
    } else { // 有数据
        process_data(buf, nbytes);
    }
    // 处理其他事情，做完了就等一会，再尝试
    nanosleep(sleep_interval, NULL);
}
```

这段代码很容易理解，就是轮询，不断的尝试有没有数据到达，有了就处理，没有(得到`EWOULDBLOCK`或者`EAGAIN`)就等一小会再试。这比之前BIO好多了，起码程序不会被卡死了。

但这样会带来**两个新问题**：

- 如果有大量文件描述符都要等，那么就得一个一个的read。这会带来大量的Context Switch（`read`是系统调用，每调用一次就得在用户态和核心态切换一次）
- 休息一会的时间不好把握。这里是要猜多久之后数据才能到。等待时间设的太长，程序响应延迟就过大；设的太短，就会造成过于频繁的重试，干耗CPU而已。

要是操作系统能一口气告诉程序，哪些数据到了就好了。

于是IO多路复用被搞出来解决这个问题。

## 四、IO多路复用

IO多路复用（IO Multiplexing) 是这么一种机制：**程序注册一组socket文件描述符给操作系统，表示“我要监视这些fd是否有IO事件发生，有了就告诉程序处理”**。

**IO多路复用是要和NIO一起使用的。尽管在操作系统级别，NIO和IO多路复用是两个相对独立的事情**。NIO仅仅是指IO API总是能立刻返回，不会被Blocking；而IO多路复用仅仅是操作系统提供的一种便利的通知机制。操作系统并不会强制这俩必须得一起用——你可以用NIO，但不用IO多路复用，就像上一节中的代码；也可以只用IO多路复用 + BIO，这时效果还是当前线程被卡住。但是，**IO多路复用和NIO是要配合一起使用才有实际意义**。因此，在使用IO多路复用之前，请总是先把fd设为`O_NONBLOCK`。

对IO多路复用，还存在一些常见的误解，比如：

- **❌IO多路复用是指多个数据流共享同一个Socket**。其实IO多路复用说的是多个Socket，只不过操作系统是一起监听他们的事件而已。

  > 多个数据流共享同一个TCP连接的场景的确是有，比如Http2 Multiplexing就是指Http2通讯中中多个逻辑的数据流共享同一个TCP连接。但这与IO多路复用是完全不同的问题。

- **❌IO多路复用是NIO，所以总是不Block的**。其实IO多路复用的关键API调用(`select`，`poll`，`epoll_wait`）总是Block的，正如下文的例子所讲。

- ❌**IO多路复用和NIO一起减少了IO**。实际上，IO本身（网络数据的收发）无论用不用IO多路复用和NIO，都没有变化。请求的数据该是多少还是多少；网络上该传输多少数据还是多少数据。IO多路复用和NIO一起仅仅是解决了调度的问题，避免CPU在这个过程中的浪费，使系统的瓶颈更容易触达到网络带宽，而非CPU或者内存。要提高IO吞吐，还是提高硬件的容量（例如，用支持更大带宽的网线、网卡和交换机）和依靠并发传输（例如HDFS的数据多副本并发传输）。

操作系统级别提供了一些接口来支持IO多路复用，最老掉牙的是`select`和`poll`。

### select

`select`长这样：

```c
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

它接受3个文件描述符的数组，分别监听读取(`readfds`)，写入(`writefds`)和异常(`expectfds`)事件。那么一个 IO多路复用的代码大概是这样：

```c
struct timeval tv = {.tv_sec = 1, .tv_usec = 0};

ssize_t nbytes;
while(1) {
    FD_ZERO(&read_fds);
    setnonblocking(fd1);
    setnonblocking(fd2);
    FD_SET(fd1, &read_fds);
    FD_SET(fd2, &read_fds);
    // 把要监听的fd拼到一个数组里，而且每次循环都得重来一次...
    if (select(FD_SETSIZE, &read_fds, NULL, NULL, &tv) < 0) { // block住，直到有事件到达
        perror("select出错了");
        exit(EXIT_FAILURE);
    }
    for (int i = 0; i < FD_SETSIZE; i++) {
        if (FD_ISSET(i, &read_fds)) {
            /* 检测到第[i]个读取fd已经收到了，这里假设buf总是大于到达的数据，所以可以一次read完 */
            if ((nbytes = read(i, buf, sizeof(buf))) >= 0) {
                process_data(nbytes, buf);
            } else {
                perror("读取出错了");
                exit(EXIT_FAILURE);
            }
        }
    }
}
```

首先，为了`select`需要构造一个fd数组（这里为了简化，没有构造要监听写入和异常事件的fd数组）。之后，用`select`监听了`read_fds`中的多个socket的读取时间。调用`select`后，程序会Block住，直到一个事件发生了，或者等到最大1秒钟(`tv`定义了这个时间长度）就返回。之后，需要遍历所有注册的fd，挨个检查哪个fd有事件到达(`FD_ISSET`返回true)。如果是，就说明数据已经到达了，可以读取fd了。读取后就可以进行数据的处理。

**`select`有一些发指的缺点**：

- `select`能够支持的最大的fd数组的长度是1024。这对要处理高并发的web服务器是不可接受的。
- fd数组按照监听的事件分为了3个数组，为了这3个数组要分配3段内存去构造，而且每次调用`select`前都要重设它们（因为`select`会改这3个数组)；调用`select`后，这3数组要从用户态复制一份到内核态；事件到达后，要遍历这3数组。很不爽。
- `select`返回后要挨个遍历fd，找到被“SET”的那些进行处理。这样比较低效。
- `select`是无状态的，即每次调用`select`，内核都要重新检查所有被注册的fd的状态。`select`返回后，这些状态就被返回了，内核不会记住它们；到了下一次调用，内核依然要重新检查一遍。于是查询的效率很低。

### poll

`poll`与`select`类似于。它大概长这样：

```c
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```

`poll`的代码例子和`select`差不多，因此也就不赘述了。有意思的是`poll`这个单词的意思是“轮询”，所以很多中文资料都会提到对IO进行“轮询”。

> **上面说的select和下文说的epoll本质上都是轮询**。

`poll`优化了`select`的一些问题。比如不再有3个数组，而是1个`polldfd`结构的数组了，并且也不需要每次重设了。数组的个数也没有了1024的限制。但其他的问题依旧：

- 依然是无状态的，性能的问题与`select`差不多一样；
- 应用程序仍然无法很方便的拿到那些“有事件发生的fd“，还是需要遍历所有注册的fd。

目前来看，高性能的web服务器都不会使用`select`和`poll`。他们俩存在的意义仅仅是“兼容性”，因为很多操作系统都实现了这两个系统调用。

如果是追求性能的话，在BSD/macOS上提供了kqueue api；在Salorias中提供了/dev/poll（可惜该操作系统已经凉凉)；而在Linux上提供了epoll api。它们的出现彻底解决了`select`和`poll`的问题。Java NIO，nginx等在对应的平台的上都是使用这些api实现。

因为大部分情况下我会用Linux做服务器，所以下文以Linux epoll为例子来解释多路复用是怎么工作的。

## 五、用epoll实现的IO多路复用

epoll是Linux下的IO多路复用的实现。这里单开一章是因为它非常有代表性，并且Linux也是目前最广泛被作为服务器的操作系统。细致的了解epoll对整个IO多路复用的工作原理非常有帮助。

### epoll创建

与`select`和`poll`不同，要使用epoll是需要先创建一下的。

```c
int epfd = epoll_create(10);
```

`epoll_create`在内核层创建了一个数据表，接口会返回一个“epoll的文件描述符”指向这个表。注意，接口参数是一个表达要监听事件列表的长度的数值。但不用太在意，因为epoll内部随后会根据事件注册和事件注销动态调整epoll中表格的大小。

![](assets/dc87ece2716b22ef6d8e90d5ff8f5d1a)

为什么epoll要创建一个用文件描述符来指向的表呢？这里有两个好处：

- epoll是有状态的，不像`select`和`poll`那样每次都要重新传入所有要监听的fd，这避免了很多无谓的数据复制。epoll的数据是用接口`epoll_ctl`来管理的（增、删、改）。
- epoll文件描述符在进程被fork时，子进程是可以继承的。这可以给对多进程共享一份epoll数据，实现并行监听网络请求带来便利。但这超过了本文的讨论范围，就此打住。

### 注册要监听的事件

epoll创建后，第二步是使用`epoll_ctl`接口来注册要监听的事件。

```c
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
```

其中第一个参数就是上面创建的`epfd`。第二个参数`op`表示如何对文件名进行操作，共有3种。

- `EPOLL_CTL_ADD` - 注册一个事件
- `EPOLL_CTL_DEL` - 取消一个事件的注册
- `EPOLL_CTL_MOD` - 修改一个事件的注册

第三个参数是要操作的fd，这里必须是支持NIO的fd（比如socket）。

第四个参数是一个`epoll_event`的类型的数据，表达了注册的事件的具体信息。

```c
typedef union epoll_data {
    void    *ptr;
    int      fd;
    uint32_t u32;
    uint64_t u64;
} epoll_data_t;

struct epoll_event {
    uint32_t     events;    /* Epoll events */
    epoll_data_t data;      /* User data variable */
};
```

比方说，想关注一个fd1的读取事件事件，并采用边缘触发(下文会解释什么是边缘触发），大概要这么写：

```c
struct epoll_data ev;
ev.events = EPOLLIN | EPOLLET; // EPOLLIN表示读事件；EPOLLET表示边缘触发
ev.data.fd = fd1;
```

通过`epoll_ctl`就可以灵活的注册/取消注册/修改注册某个fd的某些事件。

![](assets/6463bccc4ff0aadd10e03a447164c5c5)

### 管理fd事件注册

第三步，使用`epoll_wait`来等待事件的发生。

```c
int epoll_wait(int epfd, struct epoll_event *evlist, int maxevents, int timeout);
```

特别留意，**这一步是”block”的**。只有当注册的事件至少有一个发生，或者`timeout`达到时，该调用才会返回。这与`select`和`poll`几乎一致。但不一样的地方是`evlist`，它是`epoll_wait`的返回数组，里面**只包含那些被触发的事件对应的fd**，而不是像`select`和`poll`那样返回所有注册的fd。

![](assets/3f467859f3b4d959cac724dcd5d627c3)

### 监听fd事件

综合起来，一段比较完整的epoll代码大概是这样的。

```c
#define MAX_EVENTS 10
struct epoll_event ev, events[MAX_EVENTS];
int nfds, epfd, fd1, fd2;

// 假设这里有两个socket，fd1和fd2，被初始化好。
// 设置为non blocking
setnonblocking(fd1);
setnonblocking(fd2);

// 创建epoll
epfd = epoll_create(MAX_EVENTS);
if (epollfd == -1) {
    perror("epoll_create1");
    exit(EXIT_FAILURE);
}

//注册事件
ev.events = EPOLLIN | EPOLLET;
ev.data.fd = fd1;
if (epoll_ctl(epollfd, EPOLL_CTL_ADD, fd1, &ev) == -1) {
    perror("epoll_ctl: error register fd1");
    exit(EXIT_FAILURE);
}
if (epoll_ctl(epollfd, EPOLL_CTL_ADD, fd2, &ev) == -1) {
    perror("epoll_ctl: error register fd2");
    exit(EXIT_FAILURE);
}

// 监听事件
for (;;) {
    nfds = epoll_wait(epdf, events, MAX_EVENTS, -1);
    if (nfds == -1) {
        perror("epoll_wait");
        exit(EXIT_FAILURE);
    }

    for (n = 0; n < nfds; ++n) { // 处理所有发生IO事件的fd
        process_event(events[n].data.fd);
        // 如果有必要，可以利用epoll_ctl继续对本fd注册下一次监听，然后重新epoll_wait
    }
}
```

此外，[epoll的手册](https://link.jianshu.com/?t=http%3A%2F%2Fman7.org%2Flinux%2Fman-pages%2Fman7%2Fepoll.7.html) 中也有一个简单的例子。

所有的基于IO多路复用的代码都会遵循这样的写法：**注册——监听事件——处理——再注册，无限循环下去**。

## 六、epoll的优势

为什么epoll的性能比`select`和`poll`要强呢？ `select`和`poll`每次都需要把完成的fd列表传入到内核，迫使内核每次必须从头扫描到尾。而epoll完全是反过来的。epoll在内核的数据被建立好了之后，每次某个被监听的fd一旦有事件发生，内核就直接标记之。`epoll_wait`调用时，会尝试直接读取到当时已经标记好的fd列表，如果没有就会进入等待状态。

同时，`epoll_wait`直接只返回了被触发的fd列表，这样上层应用写起来也轻松愉快，再也不用从大量注册的fd中筛选出有事件的fd了。

简单说就是`select`和`poll`的代价是**“O(所有注册事件fd的数量)”**，而epoll的代价是**“O(发生事件fd的数量)”**。于是，高性能网络服务器的场景特别适合用epoll来实现——因为大多数网络服务器都有这样的模式：同时要监听大量（几千，几万，几十万甚至更多）的网络连接，但是短时间内发生的事件非常少。

但是，假设发生事件的fd的数量接近所有注册事件fd的数量，那么epoll的优势就没有了，其性能表现会和`poll`和`select`差不多。

epoll除了性能优势，还有一个优点——同时支持**水平触发(Level Trigger)**和**边沿触发(Edge Trigger)**。

### 水平触发和边沿触发

**默认情况下，epoll使用水平触发**，这与`select`和`poll`的行为完全一致。在水平触发下，epoll顶多算是一个“跑得更快的poll”。

而一旦在注册事件时使用了`EPOLLET`标记（如上文中的例子），那么将其视为**边沿触发**（或者有地方叫**边缘触发**，一个意思）。那么到底什么水平触发和边沿触发呢？

考虑下图中的例子。有两个socket的fd——fd1和fd2。我们设定监听fd1的“水平触发读事件“，监听fd2的”边沿触发读事件“。我们使用在时刻t1，使用`epoll_wait`监听他们的事件。在时刻t2时，两个fd都到了100bytes数据，于是在时刻t3, `epoll_wait`返回了两个fd进行处理。在t4，我们故意不读取所有的数据出来，只各自读50bytes。然后在t5重新注册两个事件并监听。在t6时，只有fd1会返回，因为fd1里的数据没有读完，仍然处于“被触发”状态；而fd2不会被返回，因为没有新数据到达。

![](assets/f71f027e24c130b81128a3690cf53324)

上面这个例子很明确的显示了这个例子很明确的显示了水平触发和边沿触发的区别。

- **水平触发只关心文件描述符中是否还有没完成处理的数据**，如果有，不管怎样`epoll_wait`，总是会被返回。简单说——**水平触发代表了一种“状态”**。

- **边沿触发只关心文件描述符是否有新的事件产生**，如果有，则返回；如果返回过一次，不管程序是否处理了，只要没有新的事件产生，`epoll_wait`不会再认为这个fd被“触发”了。简单说——**边沿触发代表了一个“事件”**。

  > 那么边沿触发怎么才能迫使新事件产生呢？一般需要反复调用`read`/`write`这样的IO接口，直到得到了`EAGAIN`错误码，再去尝试`epoll_wait`才有可能得到下次事件。

### 为什么需要边沿触发呢？

边沿触发把如何处理数据的控制权完全交给了开发者，提供了巨大的灵活性。比如，读取一个http的请求，开发者可以决定只读取http中的headers数据就停下来，然后根据业务逻辑判断是否要继续读（比如需要调用另外一个服务来决定是否继续读）。而不是次次被socket尚有数据的状态烦扰；写入数据时也是如此。比如希望将一个资源A写入到socket。当socket的buffer充足时，`epoll_wait`会返回这个fd是准备好的，但是资源A此时不一定准备好。如果使用水平触发，每次经过`epoll_wait`也总会被打扰。在边沿触发下，开发者有机会更精细的定制这里的控制逻辑。

但不好的一面时，边沿触发也大大的提高了编程的难度。一不留神，可能就会miss掉处理部分socket数据的机会。如果没有很好的根据`EAGAIN`来“重置”一个fd，就会造成此fd永远没有新事件产生，进而导致饿死相关的处理代码。



## 七、再来思考一下什么是“Block”

上面的所有介绍都在围绕如何让网络IO不会被Block。但是网络IO处理仅仅是整个数据处理中的一部分。如果你留意到上文例子中的“处理事件”代码，就会发现这里可能是有问题的。

- **处理代码有可能需要读写文件，可能会很慢，从而干扰整个程序的效率；**
- **处理代码有可能是一段复杂的数据计算，计算量很大的话，就会卡住整个执行流程；**
- **处理代码有bug，可能直接进入了一段死循环……**

这时你会发现，这里的Block和本文之初讲的`O_NONBLOCK`是不同的事情。在一个网络服务中，如果处理程序的延迟远远小于网络IO，那么这完全不成问题。但是如果处理程序的延迟已经大到无法忽略了，就会对整个程序产生很大的影响。这时IO多路复用已经不是问题的关键。

试分析和比较下面两个场景：

- web proxy。程序通过IO多路复用接收到了请求之后，直接转发给另外一个网络服务。
- web server。程序通过IO多路复用接收到了请求之后，需要读取一个文件，并返回其内容。

它们有什么不同？它们的瓶颈可能出在哪里？

## 八、总结

小结一下本文：

- 对于socket的文件描述符才有所谓BIO和NIO。
- 多线程+BIO模式会带来大量的资源浪费，而NIO+IO多路复用可以解决这个问题。
- 在Linux下，基于epoll的IO多路复用是解决这个问题的最佳方案；epoll相比`select`和`poll`有很大的性能优势和功能优势，适合实现高性能网络服务。

但是IO多路复用仅仅是解决了一部分问题，另外一部分问题如何解决呢？且听下回分解。

## 阅读原文

- [结合代码详细聊聊 Java 网络编程中的 BIO、NIO 和 AIO](https://www.iocoder.cn/Fight/BIO-NIO-and-AIO-in-Java-network-programming/)

